# -*- coding: utf-8 -*-
"""Rakshita Patil Case study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JdL7iBvX1TP6JPPoAwdsq0qPylYP8HCZ
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

"""Problem 1: Build a machine Learning model to predict location from an address"""

df10=pd.read_excel("Problem_1_2_4_Dataset.xlsx")

df10.isnull().sum()

df10["LOCALITY"].value_counts()

df10["LOCALITY"]=df10["LOCALITY"].replace("pune","Pune")
df10

"""Libraries for text preproccesing"""

import nltk
from nltk.tokenize import word_tokenize
nltk.download("punkt")
from nltk.corpus import stopwords
nltk.download("stopwords")
from nltk.stem import PorterStemmer,WordNetLemmatizer
nltk.download("wordnet")
ps=PorterStemmer()
lemma=WordNetLemmatizer()

"""Converting Marathi to English"""

#regex to find marathi char in df
pattern=r"[\u0900-\u097F]"            #UNICODE FOR MARATHI CHARACTERS
marathi_address=df10[df10["FORMATTED_ADDRESS"].str.contains(pattern)]
marathi_address

from google.cloud import translate_v2
translate_client = translate_v2.Client()
def translate_text(text):
    result = translate_client.translate(text, target_language='en')
    return result['input'], result['translatedText']

# df10["FORMATTED_ADDRESS"]=df10["FORMATTED_ADDRESS"].apply(translate_text)

# marathi_address["FORMATTED_ADDRESS"]=marathi_address["FORMATTED_ADDRESS"].apply(translate_text)

def clean_sent(text): 
  #tokenization and case conversion
  token=word_tokenize(text.lower())
  #token--->list of tokens
  #removing non alpha char
  ftoken=[i for i in token if i.isalpha()]
  #ftoken-----> list
  sw=stopwords.words("english")
  stokens=[i for i in ftoken if i not in sw]
  #stokens--->list
  #lemmatization
  lemma=WordNetLemmatizer()
  ltoken=[lemma.lemmatize(i) for i in stokens]
  #ltoken--->list
  #joining all tokens
  return " ".join(ltoken)
  text+=1

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier

"""Separating target and features"""

x=df10["FORMATTED_ADDRESS"]
y10=df10["LOCALITY"]

x10=[clean_sent(i) for i in x]
x10
from sklearn.model_selection import train_test_split
xtrain10,xtest10,ytrain10,ytest10=train_test_split(x10,y10,test_size=0.3,random_state=1)

"""Random Forest Classifier  (works well on imbalanced data)"""

pipe=Pipeline([
    
    ("vec",TfidfVectorizer()),
    ("rf",RandomForestClassifier())
])

pipe.fit(xtrain10, ytrain10)
ypred10_rf = pipe.predict(xtest10)
from sklearn.metrics import classification_report
print(classification_report(ytest10, ypred10_rf))

"""Prediction using Random Forest Classifier"""

def predict(text,model):
  pipe=Pipeline([
    
    ("vec",TfidfVectorizer()),
    ("rf",model)])
  pipe.fit(xtrain10, ytrain10)
  ypred10 = pipe.predict([text])[0]
  return ypred10

address="A-1/403, wing,Shraddha Saburi Tower, Vitawa ,Thane near mumbai"
adress2="4 th floor jaitu Apt. Pimpri-Chinchwad ,pune"
print(f"Result of address 1:\n{address}\nCity:{predict(address,RandomForestClassifier())}")
print(f"Result of address 2:\n{adress2}\nCity:{predict(adress2,RandomForestClassifier())}")
address3="4 th floor jaitu Apt.Pune wadi Pimpri-Chinchwad ,mumbai metropolitan"
print(f"Result of address 2:\n{address3}\nCity:{predict(address3,RandomForestClassifier())}")

"""Naive Bayes Algorithm (as it works on conditional Probability)"""

from sklearn.naive_bayes import MultinomialNB
pipe_nb=Pipeline([
    ("vec",TfidfVectorizer()),
    ("nb",MultinomialNB())
])
pipe_nb.fit(xtrain10,ytrain10)
ypred_nb=pipe_nb.predict(xtest10)
print(classification_report(ytest10, ypred_nb))

address="A-1/403, wing,Shraddha Saburi Tower, Vitawa ,Thane near mumbai"
adress2="4 th floor jaitu Apt. Pimpri-Chinchwad ,pune"
print(f"Result of address 1:\n{address}\nCity:{predict(address,MultinomialNB())}")
print(f"Result of address 2:\n{adress2}\nCity:{predict(adress2,MultinomialNB())}")
address3="4 th floor jaitu Apt.Pune wadi Pimpri-Chinchwad ,mumbai metropolitan"
print(f"Result of address 2:\n{address3}\nCity:{predict(address3,MultinomialNB())}")

"""Naive Bayes does not work that accurate but Random Forest does.
#**To improve accuracy we need our data to be balanced eg Pune has a very less data so the model is not getting learned on Pune which shows that its accuracy of Predicting Pune is low , so we need to have balanced dataset.**

Problem 2: Build search engine with autosuggestion

**We can build this using the concept of n grams**

Problem 3:Optimisation for faster execution
"""

df1=pd.read_excel("Problem_3_Dataset.xlsx",sheet_name="SOURCE")
df2=pd.read_excel("Problem_3_Dataset.xlsx",sheet_name="DESTINATION")
df1["SOURCE_ID"]=df1.index + 1 #will start from 1
df1.columns=["Source_Latitude",'Source_Longitude',"Source_id"]     #renaming columns
df2["Destination_id"]=df2.index + 1
df2.columns=["Destination_Latitude",'Destination_Longitude',"Destination_id"]
df1["id"]=df1.index +1
df2["id"]=df2.index +1

"""The data is too large and my PC's disk is not supporting . I will work with sample"""

dfnew=df1.iloc[17:23,:]
df=pd.merge(dfnew,df2.iloc[12:42,:],how="cross")

"""There are 6 records in 1st table and 30 in 2nd so total records as per condition must be 6*20=180 combinations"""

df

df=df[["Source_id",'Source_Latitude', 'Source_Longitude','Destination_id','Destination_Latitude', 'Destination_Longitude']]
df

"""The haversine library in Python is to calculate the Euclidean distance between two points given their latitude and longitude coordinates"""

# !pip install haversine
from haversine import haversine

def distance(df):
  lat1,lon1,lat2,lon2=df["Source_Latitude"],df["Source_Longitude"],df["Destination_Latitude"],df["Destination_Longitude"]

  dist=haversine((lat1,lon1),(lat2,lon2), unit="km")
  return dist

df["DISTANCE_KM"]=df.apply(distance,axis=1)

df

"""PROBLEM 4 :TF & IDF value for each word"""

data=pd.read_excel("Problem_1_2_4_Dataset.xlsx")
data

from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import word_tokenize
nltk.download("punkt")
from nltk.corpus import stopwords
nltk.download("stopwords")
from nltk.stem import PorterStemmer,WordNetLemmatizer
nltk.download("wordnet")
ps=PorterStemmer()
lemma=WordNetLemmatizer()

x=data["FORMATTED_ADDRESS"]
y=data["LOCALITY"]

stpw="""a
aadi
aaj
aap
aapne
aata
aati
aaya
aaye
ab
abbe
abbey
abe
abhi
able
about
above
accha
according
accordingly
acha
achcha
across
actually
after
afterwards
again
against
agar
ain
aint
ain't
aisa
aise
aisi
alag
all
allow
allows
almost
alone
along
already
also
although
always
am
among
amongst
an
and
andar
another
any
anybody
anyhow
anyone
anything
anyway
anyways
anywhere
ap
apan
apart
apna
apnaa
apne
apni
appear
are
aren
arent
aren't
around
arre
as
aside
ask
asking
at
aur
avum
aya
aye
baad
baar
bad
bahut
bana
banae
banai
banao
banaya
banaye
banayi
banda
bande
bandi
bane
bani
bas
bata
batao
bc
be
became
because
become
becomes
becoming
been
before
beforehand
behind
being
below
beside
besides
best
better
between
beyond
bhai
bheetar
bhi
bhitar
bht
bilkul
bohot
bol
bola
bole
boli
bolo
bolta
bolte
bolti
both
brief
bro
btw
but
by
came
can
cannot
cant
can't
cause
causes
certain
certainly
chahiye
chaiye
chal
chalega
chhaiye
clearly
c'mon
com
come
comes
could
couldn
couldnt
couldn't
d
de
dede
dega
degi
dekh
dekha
dekhe
dekhi
dekho
denge
dhang
di
did
didn
didnt
didn't
dijiye
diya
diyaa
diye
diyo
do
does
doesn
doesnt
doesn't
doing
done
dono
dont
don't
doosra
doosre
down
downwards
dude
dunga
dungi
during
dusra
dusre
dusri
dvaara
dvara
dwaara
dwara
each
edu
eg
eight
either
ek
else
elsewhere
enough
etc
even
ever
every
everybody
everyone
everything
everywhere
ex
exactly
example
except
far
few
fifth
fir
first
five
followed
following
follows
for
forth
four
from
further
furthermore
gaya
gaye
gayi
get
gets
getting
ghar
given
gives
go
goes
going
gone
good
got
gotten
greetings
haan
had
hadd
hadn
hadnt
hadn't
hai
hain
hamara
hamare
hamari
hamne
han
happens
har
hardly
has
hasn
hasnt
hasn't
have
haven
havent
haven't
having
he
hello
help
hence
her
here
hereafter
hereby
herein
here's
hereupon
hers
herself
he's
hi
him
himself
his
hither
hm
hmm
ho
hoga
hoge
hogi
hona
honaa
hone
honge
hongi
honi
hopefully
hota
hotaa
hote
hoti
how
howbeit
however
hoyenge
hoyengi
hu
hua
hue
huh
hui
hum
humein
humne
hun
huye
huyi
i
i'd
idk
ie
if
i'll
i'm
imo
in
inasmuch
inc
inhe
inhi
inho
inka
inkaa
inke
inki
inn
inner
inse
insofar
into
inward
is
ise
isi
iska
iskaa
iske
iski
isme
isn
isne
isnt
isn't
iss
isse
issi
isski
it
it'd
it'll
itna
itne
itni
itno
its
it's
itself
ityaadi
ityadi
i've
ja
jaa
jab
jabh
jaha
jahaan
jahan
jaisa
jaise
jaisi
jata
jayega
jidhar
jin
jinhe
jinhi
jinho
jinhone
jinka
jinke
jinki
jinn
jis
jise
jiska
jiske
jiski
jisme
jiss
jisse
jitna
jitne
jitni
jo
just
jyaada
jyada
k
ka
kaafi
kab
kabhi
kafi
kaha
kahaa
kahaan
kahan
kahi
kahin
kahte
kaisa
kaise
kaisi
kal
kam
kar
kara
kare
karega
karegi
karen
karenge
kari
karke
karna
karne
karni
karo
karta
karte
karti
karu
karun
karunga
karungi
kaun
kaunsa
kayi
kch
ke
keep
keeps
keh
kehte
kept
khud
ki
kin
kine
kinhe
kinho
kinka
kinke
kinki
kinko
kinn
kino
kis
kise
kisi
kiska
kiske
kiski
kisko
kisliye
kisne
kitna
kitne
kitni
kitno
kiya
kiye
know
known
knows
ko
koi
kon
konsa
koyi
krna
krne
kuch
kuchch
kuchh
kul
kull
kya
kyaa
kyu
kyuki
kyun
kyunki
lagta
lagte
lagti
last
lately
later
le
least
lekar
lekin
less
lest
let
let's
li
like
liked
likely
little
liya
liye
ll
lo
log
logon
lol
look
looking
looks
ltd
lunga
m
maan
maana
maane
maani
maano
magar
mai
main
maine
mainly
mana
mane
mani
mano
many
mat
may
maybe
me
mean
meanwhile
mein
mera
mere
merely
meri
might
mightn
mightnt
mightn't
mil
mjhe
more
moreover
most
mostly
much
mujhe
must
mustn
mustnt
mustn't
my
myself
na
naa
naah
nahi
nahin
nai
name
namely
nd
ne
near
nearly
necessary
neeche
need
needn
neednt
needn't
needs
neither
never
nevertheless
new
next
nhi
nine
no
nobody
non
none
noone
nope
nor
normally
not
nothing
novel
now
nowhere
o
obviously
of
off
often
oh
ok
okay
old
on
once
one
ones
only
onto
or
other
others
otherwise
ought
our
ours
ourselves
out
outside
over
overall
own
par
pata
pe
pehla
pehle
pehli
people
per
perhaps
phla
phle
phli
placed
please
plus
poora
poori
provides
pura
puri
q
que
quite
raha
rahaa
rahe
rahi
rakh
rakha
rakhe
rakhen
rakhi
rakho
rather
re
really
reasonably
regarding
regardless
regards
rehte
rha
rhaa
rhe
rhi
ri
right
s
sa
saara
saare
saath
sab
sabhi
sabse
sahi
said
sakta
saktaa
sakte
sakti
same
sang
sara
sath
saw
say
saying
says
se
second
secondly
see
seeing
seem
seemed
seeming
seems
seen
self
selves
sensible
sent
serious
seriously
seven
several
shall
shan
shant
shan't
she
she's
should
shouldn
shouldnt
shouldn't
should've
si
since
six
so
soch
some
somebody
somehow
someone
something
sometime
sometimes
somewhat
somewhere
soon
still
sub
such
sup
sure
t
tab
tabh
tak
take
taken
tarah
teen
teeno
teesra
teesre
teesri
tell
tends
tera
tere
teri
th
tha
than
thank
thanks
thanx
that
that'll
thats
that's
the
theek
their
theirs
them
themselves
then
thence
there
thereafter
thereby
therefore
therein
theres
there's
thereupon
these
they
they'd
they'll
they're
they've
thi
thik
thing
think
thinking
third
this
tho
thoda
thodi
thorough
thoroughly
those
though
thought
three
through
throughout
thru
thus
tjhe
to
together
toh
too
took
toward
towards
tried
tries
true
truly
try
trying
tu
tujhe
tum
tumhara
tumhare
tumhari
tune
twice
two
um
umm
un
under
unhe
unhi
unho
unhone
unka
unkaa
unke
unki
unko
unless
unlikely
unn
unse
until
unto
up
upar
upon
us
use
used
useful
uses
usi
using
uska
uske
usne
uss
usse
ussi
usually
vaala
vaale
vaali
vahaan
vahan
vahi
vahin
vaisa
vaise
vaisi
vala
vale
vali
various
ve
very
via
viz
vo
waala
waale
waali
wagaira
wagairah
wagerah
waha
wahaan
wahan
wahi
wahin
waisa
waise
waisi
wala
wale
wali
want
wants
was
wasn
wasnt
wasn't
way
we
we'd
well
we'll
went
were
we're
weren
werent
weren't
we've
what
whatever
what's
when
whence
whenever
where
whereafter
whereas
whereby
wherein
where's
whereupon
wherever
whether
which
while
who
whoever
whole
whom
who's
whose
why
will
willing
with
within
without
wo
woh
wohi
won
wont
won't
would
wouldn
wouldnt
wouldn't
y
ya
yadi
yah
yaha
yahaan
yahan
yahi
yahin
ye
yeah
yeh
yehi
yes
yet
you
you'd
you'll
your
you're
yours
yourself
yourselves
you've
yup"""

stpw=stpw.split()

stpw

def clean_sent(text): 
  #tokenization and case conversion
  token=word_tokenize(text.lower())
  #token--->list of tokens
  #removing non alpha char
  ftoken=[i for i in token if i.isalpha()]
  #ftoken-----> list
  sw=stopwords.words("english")
  
  stokens=[i for i in ftoken if i not in sw or stpw]
  #stokens--->list
  #lemmatization
  lemma=WordNetLemmatizer()
  ltoken=[lemma.lemmatize(i) for i in stokens]
  #ltoken--->list
  #joining all tokens
  return " ".join(ltoken)
  text+=1

x=[clean_sent(i) for i in x]
x

vec=TfidfVectorizer()
tf_idf_matrix=vec.fit_transform(x)

feature=vec.get_feature_names_out()
idf=vec.idf_
idf

for i,text in enumerate(x):
  tf_idf=tf_idf_matrix[i].toarray()[0]
  d={}
  d["Location"]=data["LOCALITY"]
  locations = list(set(data['LOCALITY']))
  loc_id_map = {loc: i+1 for i, loc in enumerate(locations)}
  data['Location_id'] = data['LOCALITY'].map(loc_id_map)
 
  for j,k in enumerate(tf_idf):
    if k>0:
      word=feature[j]
      d["word"]=word
      d["term frequency (TF)"]= k
      d["Inverse document frequency(IDF)"] = idf[j]

Freq_data=pd.DataFrame(d)

Freq_data

data

Freq_data["Location_id"]=data["Location_id"]
Freq_data

